# Course Overview

## Table of Contents

1. [Introduction to RL and Deep RL](course_content/1_Introduction_to_RL/Introduction.md)
2. [Q-learning](course_content/2_Q_learning/Q_Learning.md)
3. [Pytorch](course_content/3_Pytorch/Pytorch.md)
4. [Deep Q-learning](course_content/4_Deep_Q_learning/Deep_Q_Learning.md)
5. [Policy Gradient in RL](course_content/5_Policy_Gradient/Policy_Gradient.md)
6. [Proximal Policy Optimization (PPO)](course_content/6_Proximal_Policy_Optimization/PPO.md)
7. RL and Myosuite

## Prerequisite

Before we start, there are several dependencies and processes that have to be set up before training your agent. Head over to the installation page for instructions on how to set up your system.
- [Installation](installation.md)

## Homework Guidelines

Although RL code and libraries are easily avaliable online, you should not just copy and use them in your own code. Reimplementing these algorithms from scratch is a very good way to understand how these algorithms work. Below are some ways you can check your understanding of the code:

1. **Explain your algorithm and data flow** : You should be able to explain how the implemented algorithm processes observations and generates actions based it policy (e.g. taking a greedy step, or sampling distributions of past actions, etc.). Another good way is to write pseudo-code from your own implementation and compare them to the supplementary reading materials for verification.

2. **Reimplement algorithms** : As mentioned above, reimplementing from scratch is a good way to understand the algorithms. However, it is understandable that this can be a daunting task, hence, code references will be provided to aid in your implementation. But, do not copy and paste.

3. **Comparison with existing libraries** : Comparing your implementation with existing baseline libraries will help you ensure that your algorithm is implemented correctly. Note that you don't have to ensure your output is exactly the same as the libraries, given the stochastic nature of RL training. You just need to check if your algorithm is having a similar learning curve and has a similar action sequence on the same problems.


## Example Curriculum

You are expected to spend about 10 hours per week for this study group (2 hours weekly meetings/discussions, with 8 hours studying and homework). The schedule below is a rough guide on how you can schedule your time for contents of the study group

**Week 0:** [Introduction to RL and Deep RL](course_content/1_Introduction_to_RL/Introduction.md)

**Week 1:** [Q-learning](course_content/2_Q_learning/Q_Learning.md)

**Week 2:** [Pytorch](course_content/3_Pytorch/Pytorch.md)

**Week 3:** [Deep Q-learning](course_content/4_Deep_Q_learning/Deep_Q_Learning.md)

**Week 4:** [Policy Gradient in RL](course_content/5_Policy_Gradient/Policy_Gradient.md)

**Week 5:** [Introduction to Proximal Policy Optimization (PPO)](course_content/6_Proximal_Policy_Optimization/PPO.md)

**Week 6:** [Proximal Policy Optimization (PPO)](course_content/6_Proximal_Policy_Optimization/PPO.md)

**Week 7:** Myosuite

**Week 8:** Myosuite and Deep RL

Happy studying!
